# RAG-EVALUATE

Pre-requisites to execute the code..

Download ollama and models
First, let's start by installing ollama from project's website: ollama.com or https://ollama.com/download

After installed, open a terminal and run the following command to download the required models:

ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf
ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF

If you see the following output, it means the models are successfully downloaded:

pulling manifest
...
verifying sha256 digest
writing manifest
success

Before continuing, to use ollama in python, let's also install the ollama package:

pip install ollama

Get the data from https://huggingface.co/ngxson/demo_simple_rag_py/blob/main/cat-facts.txt

The code is included in jupyter notebook.
Incase of other IDEs, copy the code block, paste and simply run.

Points to remember:

1) This is a basic RAG with similarity as a preferred metric for Evaluation

2) I chose this metric (Generation Evaluation --> Answer Similarity) amongst others as it calculates how semantically similar are the generated answer and the ground truth. In simple terms, how these two are similar conceptually. For query: “In which countries are snow leopards found?” the generated answer might mention only a few countries with snow leopards, but it would have a high answer similarity because the answer is conceptually similar to the ground truth. Again, 1 being the highest similarity score and 0 being the lowest.

3) Other types of evalaution metrics are as follows..

a) Retrieval Evaluation: This part evaluates the context that is passed to the LLM. The metrics involved are:

Context Recall: The metric checks if all the relevant answers to the question are present in the context. For the user query: “Who discovered the Galapagos Islands and how?” A context with high recall will answer both the parts of question — Who and How. The answer to both of these questions is present in the ground truth. Hence, this metric utilizes context and ground truth to determine a score between 0 and 1. 1 being the highest recall.

Context Precision: This metric determines whether context that is closest to the ground truth is given high score. The more relevant is the chunk to the ground truth the higher will be the score. Context Precision is determined by ground truth, context, and user query. Higher the score, ranging from 0 to 1, higher the context precision.

Context Entities Recall: This recall determines whether all entities present in the ground truth are also present in the supplied context. For query: “In which countries are snow leopards found?” the ground truth mentions 12 countries. If the context contains all the names of these countries, then it would result in high context entity recall.


b) Generation Evaluation: This part evaluates the answer generated by the LLM.

Faithfulness: The metric outputs a score between 0 and 1, determining the extent to which the generated response relies solely on the provided context. The lower the score, the less trustworthy the generated answer is, the less reliance on the supplied context.

Answer Relevance: It measures how relevant and pertinent is the generated answer to the user query. For query: “What are the threats to penguin populations?” an irrelevant answer might focus on location of penguins while a relevant answer would mention the threats to penguins populations.

Answer Correctness: This metric determines how factually correct is the generated output. It utilizes the ground truth and generated answer to determine this score. The higher the better, from 0 to 1. It is not to be confused with faithfulness as an answer can be (factually) correct but can not be faithful if it is not generated using the context.

Answer Harmfulness: This metric simply determines if the output is potentially offensive to an individual, group, or a society. The output is binary, 0 or 1.

I took the reference from this blog post - https://namratanwani.medium.com/evaluate-rag-with-ragas-e1ad1aa99c2e



